{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "NLP 入門 (1-3) Word2Vec Sentiment Analysis.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tsingjinyun/----------/blob/master/NLP_%E5%85%A5%E9%96%80_(1_3)_Word2Vec_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUpWLiiV3Mm7"
      },
      "source": [
        "### 準備 Pre-trained 的 Word2Vec weights  \n",
        "歡迎參考我的這篇[blog](https://medium.com/@sfhsu29/nlp-%E5%B0%88%E6%AC%84-1-2-%E5%A6%82%E4%BD%95%E8%A8%93%E7%B7%B4%E8%87%AA%E5%B7%B1%E7%9A%84-word2vec-5a0754c5cb09)來使用維基百科的資料訓練自己的word vector  \n",
        "如果只想下載 word2vec 參數到你的local的話：[google drive連結](https://docs.google.com/u/0/uc?export=download&confirm=8s0o&id=17ThJchrSwnOy1HEKX1HT79vpl1y4UH2L)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRBC-ix9zAzy",
        "outputId": "003a6aff-170b-4875-da44-3ea94fa22672"
      },
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=17ThJchrSwnOy1HEKX1HT79vpl1y4UH2L' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=17ThJchrSwnOy1HEKX1HT79vpl1y4UH2L\" -O wiki-word2vec.zip && rm -rf /tmp/cookies.txt\n",
        "!unzip wiki-word2vec.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-27 00:16:04--  https://docs.google.com/uc?export=download&confirm=AOlH&id=17ThJchrSwnOy1HEKX1HT79vpl1y4UH2L\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.202.138, 74.125.202.102, 74.125.202.100, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.202.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0s-64-docs.googleusercontent.com/docs/securesc/sulptmrivnfhk17i3c0jcdn7davgj883/4o4anhefs66ofud344tis43flkd54jjc/1609028100000/18293686574256102848/03278925523020089386Z/17ThJchrSwnOy1HEKX1HT79vpl1y4UH2L?e=download [following]\n",
            "--2020-12-27 00:16:04--  https://doc-0s-64-docs.googleusercontent.com/docs/securesc/sulptmrivnfhk17i3c0jcdn7davgj883/4o4anhefs66ofud344tis43flkd54jjc/1609028100000/18293686574256102848/03278925523020089386Z/17ThJchrSwnOy1HEKX1HT79vpl1y4UH2L?e=download\n",
            "Resolving doc-0s-64-docs.googleusercontent.com (doc-0s-64-docs.googleusercontent.com)... 172.217.212.132, 2607:f8b0:4001:c03::84\n",
            "Connecting to doc-0s-64-docs.googleusercontent.com (doc-0s-64-docs.googleusercontent.com)|172.217.212.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://docs.google.com/nonceSigner?nonce=8er09r3tsgjq8&continue=https://doc-0s-64-docs.googleusercontent.com/docs/securesc/sulptmrivnfhk17i3c0jcdn7davgj883/4o4anhefs66ofud344tis43flkd54jjc/1609028100000/18293686574256102848/03278925523020089386Z/17ThJchrSwnOy1HEKX1HT79vpl1y4UH2L?e%3Ddownload&hash=nh1fauet4ok0qossspjn1p42jc6l7kkv [following]\n",
            "--2020-12-27 00:16:04--  https://docs.google.com/nonceSigner?nonce=8er09r3tsgjq8&continue=https://doc-0s-64-docs.googleusercontent.com/docs/securesc/sulptmrivnfhk17i3c0jcdn7davgj883/4o4anhefs66ofud344tis43flkd54jjc/1609028100000/18293686574256102848/03278925523020089386Z/17ThJchrSwnOy1HEKX1HT79vpl1y4UH2L?e%3Ddownload&hash=nh1fauet4ok0qossspjn1p42jc6l7kkv\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.202.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://doc-0s-64-docs.googleusercontent.com/docs/securesc/sulptmrivnfhk17i3c0jcdn7davgj883/4o4anhefs66ofud344tis43flkd54jjc/1609028100000/18293686574256102848/03278925523020089386Z/17ThJchrSwnOy1HEKX1HT79vpl1y4UH2L?e=download&nonce=8er09r3tsgjq8&user=03278925523020089386Z&hash=6dd7rh744te25qhtmj5gjd4an2mesgie [following]\n",
            "--2020-12-27 00:16:04--  https://doc-0s-64-docs.googleusercontent.com/docs/securesc/sulptmrivnfhk17i3c0jcdn7davgj883/4o4anhefs66ofud344tis43flkd54jjc/1609028100000/18293686574256102848/03278925523020089386Z/17ThJchrSwnOy1HEKX1HT79vpl1y4UH2L?e=download&nonce=8er09r3tsgjq8&user=03278925523020089386Z&hash=6dd7rh744te25qhtmj5gjd4an2mesgie\n",
            "Connecting to doc-0s-64-docs.googleusercontent.com (doc-0s-64-docs.googleusercontent.com)|172.217.212.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘wiki-word2vec.zip’\n",
            "\n",
            "wiki-word2vec.zip       [        <=>         ] 253.96M   178MB/s    in 1.4s    \n",
            "\n",
            "2020-12-27 00:16:06 (178 MB/s) - ‘wiki-word2vec.zip’ saved [266297069]\n",
            "\n",
            "Archive:  wiki-word2vec.zip\n",
            "  inflating: wiki-lemma-100D-phrase  \n",
            "  inflating: wiki-lemma-100D-phrase.trainables.syn1neg.npy  \n",
            "  inflating: wiki-lemma-100D-phrase.wv.vectors.npy  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wo1dadT04VMT"
      },
      "source": [
        "#### 使用 Gensim 來 Load 剛剛下載好的參數"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJ8Y0W0a3Ji3"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "model = Word2Vec.load(\"wiki-lemma-100D-phrase\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIrU8YvU4eZq"
      },
      "source": [
        "#### Google 的 word representation, word embedding, word vector (這三這幾乎是同義詞)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRsC-_ph38va",
        "outputId": "ff3969bf-a4d7-4be0-96ef-fe4046d4e983"
      },
      "source": [
        "model.wv['Google']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 3.3885376 , -4.2511983 , -1.445084  ,  1.9302932 ,  0.84648174,\n",
              "       -0.63184077,  0.05726242,  0.7063744 , -0.05059653,  2.1846352 ,\n",
              "       -0.12328646,  0.6471171 ,  0.25026178, -0.14930539, -0.07820829,\n",
              "        0.16145864,  0.6498362 ,  1.0886047 ,  1.5343726 ,  4.950689  ,\n",
              "        0.50769436, -0.3946989 ,  2.1025164 , -3.1834273 ,  2.380777  ,\n",
              "        0.24872196, -2.5802555 ,  1.4149348 ,  0.63408375, -1.6591691 ,\n",
              "       -2.4389815 ,  2.1249    ,  0.39783332, -0.35228804, -2.9626958 ,\n",
              "       -4.475956  , -4.0608654 , -0.39374784, -0.37150937, -1.4930124 ,\n",
              "       -2.4813695 ,  2.203496  ,  1.9972353 ,  5.479271  , -0.00601585,\n",
              "        2.1382024 , -1.6378103 ,  1.9081411 , -0.6341882 , -2.2414782 ,\n",
              "       -0.70867395,  1.1144645 , -1.7722839 ,  0.37062666,  0.80082476,\n",
              "        1.9306395 ,  0.48852623, -1.1423063 ,  3.0780652 ,  0.7570495 ,\n",
              "       -2.8385687 ,  0.5158689 , -1.316193  ,  0.51292163,  1.862376  ,\n",
              "        1.7524154 ,  1.8987881 ,  0.06306809,  1.378573  , -3.1835907 ,\n",
              "       -1.7185044 , -1.2889273 , -2.4795012 ,  2.7307894 , -1.8105891 ,\n",
              "       -3.9534705 , -2.8385644 ,  1.1834143 ,  1.0085803 , -0.8193638 ,\n",
              "        2.307464  , -1.1131806 ,  0.6802008 ,  0.5620667 , -5.907173  ,\n",
              "        3.8217628 ,  1.9559139 , -1.3510746 , -1.45398   , -0.5087807 ,\n",
              "       -3.1976836 ,  1.17948   , -5.1467657 ,  3.9677458 , -0.48014182,\n",
              "        2.1130106 ,  1.8481046 , -4.7859154 , -2.2084837 ,  1.1197177 ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zh9Yxeio4kGB"
      },
      "source": [
        "和Google 在向量空間裡，最靠近（cosine similarity）的單詞們！  \n",
        "在訓練時我是使用case-sensitive且包含片語的"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iU_cM3o_4O5i",
        "outputId": "73a93bec-314d-4384-8c48-99e0159c7763"
      },
      "source": [
        "model.wv.most_similar('Google', topn = 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Microsoft', 0.8559668064117432),\n",
              " ('Google_Search', 0.8424477577209473),\n",
              " ('Google_Analytics', 0.8412378430366516),\n",
              " ('Yahoo', 0.8326560854911804),\n",
              " ('AltaVista', 0.8248958587646484)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nyVbkgw69-m"
      },
      "source": [
        "### 2. 下載 sentiment analysis 所要用到的 IMDB 資料集，並解壓縮到colab drive裡面"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdi3fFjX6_NS",
        "outputId": "dad151d1-6c2a-4cde-98b8-920052cd78c2"
      },
      "source": [
        "! wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "! tar zxf aclImdb_v1.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-27 00:16:14--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz’\n",
            "\n",
            "aclImdb_v1.tar.gz   100%[===================>]  80.23M  25.0MB/s    in 4.3s    \n",
            "\n",
            "2020-12-27 00:16:19 (18.7 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Or52i4HYY2V"
      },
      "source": [
        "#### 從解壓縮的資料夾讀取 IMDB 資料"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NizAsomP7LhK"
      },
      "source": [
        "import os\n",
        "\n",
        "def read_files_to_list(path):\n",
        "  docs = []\n",
        "  # 借助os.listdir找出特定folder下所有的files\n",
        "  files = os.listdir(path)\n",
        "  for file in files:  \n",
        "    # 再把path 和 file names join起來，就可以得到我們要的檔案位置\n",
        "    with open(os.path.join(path, file)) as f:\n",
        "      docs.append(f.read())\n",
        " \n",
        "  return docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dii6-s8w7L3x"
      },
      "source": [
        "# Set the path of positive examples\n",
        "path_pos = \"aclImdb/train/pos\"\n",
        "# Set the path of negative examples\n",
        "path_neg = \"aclImdb/train/neg\"\n",
        "\n",
        "pos_file_list = read_files_to_list(path_pos)\n",
        "neg_file_list = read_files_to_list(path_neg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iVm4jdk7Ndv",
        "outputId": "2aeaf895-3352-4360-8cc0-0ad1e884def8"
      },
      "source": [
        " # 總共有25000個 training data\n",
        " len(pos_file_list+neg_file_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_LgaTXs7Piz"
      },
      "source": [
        "import pandas as pd\n",
        "# 把 positive 的文章list 和 negative 的文章 list 串接再一起，在和他們對應的label zip再一起，變成 \n",
        "# ((positive article, 1)\n",
        "#  (positive article, 1)\n",
        "#  ...\n",
        "#  (negative article, 0))\n",
        "imdb_df = pd.DataFrame(data = zip(pos_file_list +neg_file_list, [1] * len(pos_file_list) + [0] * len(neg_file_list)))\n",
        "imdb_df.columns = ['text', 'label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "PzyC5tSt7SE1",
        "outputId": "5e8b5606-dfd3-4e4c-be60-bd3bc64e084e"
      },
      "source": [
        "imdb_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Yesterday, I went to the monthly Antique Flea ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I watched \"Elephant Walk\" for the first time i...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>You probably all already know this by now, but...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I love this movie/short thing. Jason Steele is...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Great movie, enough laughs and action for any ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  label\n",
              "0  Yesterday, I went to the monthly Antique Flea ...      1\n",
              "1  I watched \"Elephant Walk\" for the first time i...      1\n",
              "2  You probably all already know this by now, but...      1\n",
              "3  I love this movie/short thing. Jason Steele is...      1\n",
              "4  Great movie, enough laughs and action for any ...      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "HTeCNT457Ttl",
        "outputId": "4765d3c0-7c75-4328-a269-32ed56e0004c"
      },
      "source": [
        "imdb_df.tail()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>24995</th>\n",
              "      <td>If you like to comment on films where the scri...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24996</th>\n",
              "      <td>The social commentary was way overblown and th...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24997</th>\n",
              "      <td>In the first Howling, we are introduced to a w...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24998</th>\n",
              "      <td>I loved Adrianne Curry before this show. I tho...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24999</th>\n",
              "      <td>&lt;br /&gt;&lt;br /&gt;I am a big-time horror/sci-fi fan ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text  label\n",
              "24995  If you like to comment on films where the scri...      0\n",
              "24996  The social commentary was way overblown and th...      0\n",
              "24997  In the first Howling, we are introduced to a w...      0\n",
              "24998  I loved Adrianne Curry before this show. I tho...      0\n",
              "24999  <br /><br />I am a big-time horror/sci-fi fan ...      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igFKVfSR79bD"
      },
      "source": [
        "#### 在我第一篇[blog](https://sfhsu29.medium.com/nlp-%E5%85%A5%E9%96%80-1-text-classification-sentiment-analysis-%E6%A5%B5%E7%B0%A1%E6%98%93%E6%83%85%E6%84%9F%E5%88%86%E9%A1%9E%E5%99%A8-bag-of-words-naive-bayes-e40d61de9a7f) 之中提到，如果使用：\n",
        "- one-hot encoding 且沒有 remove stop words: 0.849\n",
        "- one-hot encoding 且 remove stop words: 0.859\n",
        "- tf-idf encoding 且沒有 remove stop words: 0.864\n",
        "- tf-idf encoding 且 remove stop words: 0.867\n",
        "\n",
        "讓我們來測試看看 word embedding 有沒有更厲害"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z67wucgU3O9s"
      },
      "source": [
        "### Baseline: 將每個單詞的詞向量取平均，在使用 Support Vector Classifier 來分類 (0.802 accuracy)\n",
        "\n",
        "(想要使用 Deep Learning 的話，請直接跳到下一個單元)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtZk851F9iQH"
      },
      "source": [
        "#### 1. 清理資料：\n",
        "當我們訓練word2vec的時候，我們選擇了Lemmatize了wikipedia的資料，因此我們也要用相同的步驟來清理IMDB的資料"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vv_dLkoQ7YlV"
      },
      "source": [
        "import spacy\n",
        "import spacy.cli\n",
        "spacy.cli.download(\"en_core_web_md\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBUf4oEA87nr"
      },
      "source": [
        "nlp = spacy.load('en_core_web_md', disable=[\"ner\", \"parser\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bOxVp4CIrWH"
      },
      "source": [
        "#### 2. 處理文字：\n",
        "\n",
        "\n",
        "1.   使用 spacy 的 lemma_ 來取得單詞 lemmatize 的結果\n",
        "2.   使用 w2vmodel.wv[word.lemma_] 來取得該單詞的 word embeddings 最後再將整個句子的 word embeddings 取平均。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRmcnRuu358b"
      },
      "source": [
        "# 示範一下 lemma 的結果\n",
        "for token in  nlp(\"Apple is looking at buying U.K. startup for $1 billion\"):\n",
        "  print('Word: {}  --> {}'.format(token.text, token.lemma_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7ju7d_s9uGv"
      },
      "source": [
        "import numpy as np\n",
        "# Get all words from wiki word2vec model\n",
        "model_words = set(model.wv.index2word)\n",
        "\n",
        "def text2vec_raw(w2vmodel, text):\n",
        "  doc = nlp(text)\n",
        "  # convert a movie review into vectors\n",
        "  text_vecs = [w2vmodel.wv[word.lemma_] for word in doc if word.lemma_ in model_words]\n",
        "  # calculate the mean of the vectors and return\n",
        "  if len(text_vecs) > 1:\n",
        "    res =  np.mean(text_vecs, axis = 0)\n",
        "    return res\n",
        "  elif len(text_vecs) == 1:\n",
        "    return res[0]\n",
        "  else:\n",
        "    return np.nan\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56NseBAE_PHP"
      },
      "source": [
        "imdb_df['vectors'] = imdb_df['text'].apply(lambda x: text2vec_raw(model, x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOSRhP7WDaan"
      },
      "source": [
        "imdb_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIJ9RJaUQK3L"
      },
      "source": [
        "# 找出是否有 label 不為 pos, neg (沒有)\n",
        "imdb_df[~imdb_df['label'].isin(['pos', 'neg'])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJvJ060jIvgw"
      },
      "source": [
        "vector_df = pd.DataFrame(imdb_df.vectors.tolist())\n",
        "vector_df['label'] = imdb_df['label'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQC-C7OFMAwS"
      },
      "source": [
        "# 使用 Sample 來 shuffle data\n",
        "vector_df = vector_df.sample(frac =1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9nmXnDY5iMB"
      },
      "source": [
        "#### 3. 建立一個 model 來預測句子的sentiment (scikit learn, Machine Learning)\n",
        "\n",
        "\n",
        "1.   RandomForestClassifier = 0.757 (可能略有不同)\n",
        "2.   GradientBoostingClassifier = 0.772 (可能略有不同)\n",
        "3.   SVM classifier = 0.801 (可能略有不同)\n",
        "\n",
        "-> 可以發現模型的正確率並沒有比 Bag-of-words的方法來的好  \n",
        "-> 錯誤分析: 在之前的步驟之中，我們假設 sentence embedding = average of word embeddings，這個取平均的動作，會讓 embeddings 之間的「特性」被互相抵消。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iizIRScL8OaU"
      },
      "source": [
        "#### 1. Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUWRKOtfMNDF"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoTSwkTsNSIq"
      },
      "source": [
        "clf = RandomForestClassifier()\n",
        "Y = vector_df['label'].values\n",
        "X = vector_df.drop(['label'], axis = 1)\n",
        " \n",
        "scores = cross_val_score(clf, X, Y, cv = 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfIN-7hLQc2b"
      },
      "source": [
        "print(scores.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8bsOYmI8VpZ"
      },
      "source": [
        "#### 2. Gradient Boosting Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zw8pkdOSQySo"
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iz2jfEjZSS5l"
      },
      "source": [
        "clf = GradientBoostingClassifier()\n",
        "Y = vector_df['label'].values\n",
        "X = vector_df.drop(['label'], axis = 1)\n",
        " \n",
        "scores = cross_val_score(clf, X, Y, cv = 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4jS56AwSWs-"
      },
      "source": [
        "print(scores.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpyOZUbq8ZKf"
      },
      "source": [
        "#### 3. Support Vector Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDcUXAfTSXrP"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "clf = SVC()\n",
        "Y = vector_df['label'].values\n",
        "X = vector_df.drop(['label'], axis = 1)\n",
        " \n",
        "scores = cross_val_score(clf, X, Y, cv = 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43MKXCkUSxes"
      },
      "source": [
        "print(scores.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-5ZWPS17Ndd"
      },
      "source": [
        "### 建立一個 model 來預測句子的 sentiment (keras, deep learning)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbTAeMJi_C26"
      },
      "source": [
        "# 由於我們剛剛下載的 word2vec model 是專門給 gensim 用的，如果我們要給 keras 使用，我們要先將model儲存成不同的格式\n",
        "model.wv.save_word2vec_format('keras_word2vec.txt', binary=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKQ91bxUF4EX"
      },
      "source": [
        "#### 將 word embeddings 儲存到字典(dictionary)裡面"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V39u29Hp_rOT",
        "outputId": "bb513987-213a-4239-bf8c-76d8e26271cb"
      },
      "source": [
        "import numpy as np\n",
        "# load the whole embedding into memory\n",
        "embeddings_index = dict()\n",
        "with open('keras_word2vec.txt') as f:\n",
        "  for line in f:\n",
        "    values = line.split()\n",
        "    # 只接受長度為 101 的向量 (word + 100d embedding)\n",
        "    if len(values) != 101:\n",
        "      print(values)\n",
        "      continue\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['346848', '100']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hepUhbhnGBkn"
      },
      "source": [
        "#### 找出適合的句子長度：\n",
        "(最後使用 max_len = 512)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3T3_vYHKEltP",
        "outputId": "8874d6ac-ef05-4219-f919-22d6579cc1fc"
      },
      "source": [
        "import numpy as np\n",
        "### 找出影評共有幾個 words\n",
        "sentence_lengths = imdb_df['text'].apply(lambda x: len(x.split()))\n",
        "quantiles = np.percentile(sentence_lengths, [25, 50, 75])\n",
        "\n",
        "print('The minimal word count in a movie review: ', min(sentence_lengths))\n",
        "print('The first quantile (25%) of word count in a movie review:', quantiles[0])\n",
        "print('The second quantile (50%) of word count in a movie review:', quantiles[1])\n",
        "print('The third quantile (75%) of word count in a movie review:', quantiles[2])\n",
        "print('The maximum word count in a movie review: ', max(sentence_lengths))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The minimal word count in a movie review:  10\n",
            "The first quantile (25%) of word count in a movie review: 127.0\n",
            "The second quantile (50%) of word count in a movie review: 174.0\n",
            "The third quantile (75%) of word count in a movie review: 284.0\n",
            "The maximum word count in a movie review:  2470\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1PkAXFAZahH"
      },
      "source": [
        "#### 處理文字 (lemmatize)，並儲存到review_lines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnCUb0Ycsi4J",
        "outputId": "ec9b3be4-8fda-4c40-8640-a55b40393eb6"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import spacy.cli\n",
        "spacy.cli.download(\"en_core_web_md\")\n",
        "nlp = spacy.load('en_core_web_md', disable=[\"ner\", \"parser\"])\n",
        "\n",
        "review_lines = list()\n",
        "reviews = imdb_df['text'].values.tolist()\n",
        "\n",
        "for review in tqdm(reviews):   \n",
        "    doc = nlp(review)\n",
        "    words = [word.lemma_ for word in doc]\n",
        "    review_lines.append(words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 25000/25000 [16:17<00:00, 25.57it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4e5MJluflgw",
        "outputId": "8117dc14-b255-439d-c9ae-8cbe6fc672e0"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# 初始化 tokenizer\n",
        "# 將文字轉換成 2D integer tensor\n",
        "tokenizer_obj = Tokenizer()\n",
        "tokenizer_obj.fit_on_texts(review_lines)\n",
        "sequences = tokenizer_obj.texts_to_sequences(review_lines)\n",
        "\n",
        "# 因為model的input必須一樣長，所以要使用padding，句子短於512的都會補0，長於512的會被truncate\n",
        "word_index = tokenizer_obj.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "review_pad = pad_sequences(sequences, maxlen=512)\n",
        "\n",
        "sentiment =  imdb_df['label'].values\n",
        "print('Shape of review tensor:', review_pad.shape)\n",
        "print('Shape of sentiment tensor:', sentiment.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 90801 unique tokens.\n",
            "Shape of review tensor: (25000, 512)\n",
            "Shape of sentiment tensor: (25000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVKLcVtz9b8g"
      },
      "source": [
        "# 建立一個 look up table，\b第一個 row 就是第一個單詞的 word embedding\n",
        "vocab_size = len(word_index) + 1\n",
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "for word, i in tokenizer_obj.word_index.items():\n",
        "\tembedding_vector = embeddings_index.get(word)\n",
        "\tif embedding_vector is not None:\n",
        "\t\tembedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJ1iwMAto5JK",
        "outputId": "88be9c44-e9f4-41b0-9e12-f5264c318265"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import GRU\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from keras.initializers import Constant\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# define model\n",
        "gru_model = Sequential()\n",
        "e = Embedding(vocab_size, 100, embeddings_initializer=Constant(embedding_matrix), trainable=False)\n",
        "gru_model.add(e)\n",
        "gru_model.add(GRU(64, dropout = 0.2, recurrent_dropout = 0.2))\n",
        "gru_model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "gru_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# summarize the model\n",
        "print(gru_model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, None, 100)         9080200   \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, 64)                31872     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 9,112,137\n",
            "Trainable params: 31,937\n",
            "Non-trainable params: 9,080,200\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qz-wXKq7I0JL",
        "outputId": "35563c43-6775-4bce-c2b4-18d51a26855d"
      },
      "source": [
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
        "# fit the model\n",
        "gru_model.fit(review_pad, sentiment, batch_size=128, epochs=10, validation_split=0.1, verbose=1, callbacks=[tensorboard_callback])\n",
        "# evaluate the model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "176/176 [==============================] - 198s 1s/step - loss: 0.6650 - accuracy: 0.5909 - val_loss: 0.5415 - val_accuracy: 0.7548\n",
            "Epoch 2/10\n",
            "176/176 [==============================] - 188s 1s/step - loss: 0.4882 - accuracy: 0.7737 - val_loss: 0.3647 - val_accuracy: 0.8408\n",
            "Epoch 3/10\n",
            "176/176 [==============================] - 187s 1s/step - loss: 0.3976 - accuracy: 0.8280 - val_loss: 0.3257 - val_accuracy: 0.8832\n",
            "Epoch 4/10\n",
            "176/176 [==============================] - 192s 1s/step - loss: 0.3512 - accuracy: 0.8478 - val_loss: 0.3375 - val_accuracy: 0.8744\n",
            "Epoch 5/10\n",
            "176/176 [==============================] - 189s 1s/step - loss: 0.3259 - accuracy: 0.8611 - val_loss: 0.2907 - val_accuracy: 0.8900\n",
            "Epoch 6/10\n",
            "176/176 [==============================] - 192s 1s/step - loss: 0.3013 - accuracy: 0.8723 - val_loss: 0.3731 - val_accuracy: 0.8416\n",
            "Epoch 7/10\n",
            "176/176 [==============================] - 190s 1s/step - loss: 0.2928 - accuracy: 0.8763 - val_loss: 0.2536 - val_accuracy: 0.8968\n",
            "Epoch 8/10\n",
            "176/176 [==============================] - 188s 1s/step - loss: 0.2853 - accuracy: 0.8806 - val_loss: 0.2008 - val_accuracy: 0.9196\n",
            "Epoch 9/10\n",
            "176/176 [==============================] - 192s 1s/step - loss: 0.2765 - accuracy: 0.8868 - val_loss: 0.3049 - val_accuracy: 0.8808\n",
            "Epoch 10/10\n",
            "176/176 [==============================] - 192s 1s/step - loss: 0.2630 - accuracy: 0.8920 - val_loss: 0.3735 - val_accuracy: 0.8408\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7b94cff0b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jzOxdPHAkBu"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 821
        },
        "id": "pr9KB-gWARnq",
        "outputId": "93db2404-e705-4c34-c424-17a107c2f99c"
      },
      "source": [
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43m此单元格的输出内容太大，只能在登录的情况下显示。\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_Q4sjlskzAL"
      },
      "source": [
        "實驗一：使用隨機 embedding layer 初始值"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Iz4f8VvN7H9",
        "outputId": "05d09a79-635e-4cbe-a4ad-fb3afd0113b7"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import GRU\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from keras.initializers import Constant\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# define model\n",
        "model = Sequential()\n",
        "e = Embedding(vocab_size, 100, trainable=False)\n",
        "model.add(e)\n",
        "model.add(GRU(64, dropout = 0.2, recurrent_dropout = 0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# summarize the model\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, None, 100)         9080200   \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, 64)                31872     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 9,112,137\n",
            "Trainable params: 31,937\n",
            "Non-trainable params: 9,080,200\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6W3WYNpDN9pc",
        "outputId": "bf02d198-085a-4f85-c8fa-baba955c1b5f"
      },
      "source": [
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
        "# fit the model\n",
        "model.fit(review_pad, sentiment, batch_size=128, epochs=10, validation_split=0.1, verbose=1, callbacks=[tensorboard_callback])\n",
        "# evaluate the model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "176/176 [==============================] - 203s 1s/step - loss: 0.6848 - accuracy: 0.5590 - val_loss: 0.7266 - val_accuracy: 0.3812\n",
            "Epoch 2/10\n",
            "176/176 [==============================] - 200s 1s/step - loss: 0.6669 - accuracy: 0.5947 - val_loss: 0.7439 - val_accuracy: 0.4416\n",
            "Epoch 3/10\n",
            "176/176 [==============================] - 199s 1s/step - loss: 0.6384 - accuracy: 0.6369 - val_loss: 0.8711 - val_accuracy: 0.1556\n",
            "Epoch 4/10\n",
            "176/176 [==============================] - 199s 1s/step - loss: 0.6437 - accuracy: 0.6384 - val_loss: 0.7835 - val_accuracy: 0.4760\n",
            "Epoch 5/10\n",
            "176/176 [==============================] - 200s 1s/step - loss: 0.6097 - accuracy: 0.6666 - val_loss: 0.7672 - val_accuracy: 0.4128\n",
            "Epoch 6/10\n",
            "176/176 [==============================] - 199s 1s/step - loss: 0.6199 - accuracy: 0.6606 - val_loss: 0.7508 - val_accuracy: 0.5308\n",
            "Epoch 7/10\n",
            "176/176 [==============================] - 199s 1s/step - loss: 0.5891 - accuracy: 0.6886 - val_loss: 0.6567 - val_accuracy: 0.6256\n",
            "Epoch 8/10\n",
            "176/176 [==============================] - 200s 1s/step - loss: 0.5808 - accuracy: 0.6998 - val_loss: 0.6405 - val_accuracy: 0.6340\n",
            "Epoch 9/10\n",
            "176/176 [==============================] - 198s 1s/step - loss: 0.5666 - accuracy: 0.7079 - val_loss: 0.6973 - val_accuracy: 0.5748\n",
            "Epoch 10/10\n",
            "176/176 [==============================] - 199s 1s/step - loss: 0.5587 - accuracy: 0.7162 - val_loss: 0.7315 - val_accuracy: 0.5448\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb315466588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 838
        },
        "id": "owDBFmLxqqyr",
        "outputId": "7ef6a014-139d-42a3-b921-68db79b6c245"
      },
      "source": [
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43m此单元格的输出内容太大，只能在登录的情况下显示。\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmwO95uDOHo5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04e18308-2150-4097-a561-d9e73f7765e2"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import GRU\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from keras.initializers import Constant\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# define model\n",
        "model = Sequential()\n",
        "e = Embedding(vocab_size, 100, trainable=True)\n",
        "model.add(e)\n",
        "model.add(GRU(64, dropout = 0.2, recurrent_dropout = 0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# summarize the model\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, None, 100)         9080200   \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  (None, 64)                31872     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 9,112,137\n",
            "Trainable params: 9,112,137\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrpYAXjmqqEQ",
        "outputId": "2507d3af-05c3-4e1f-b63c-b355508bd20a"
      },
      "source": [
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
        "# fit the model\n",
        "model.fit(review_pad, sentiment, batch_size=128, epochs=10, validation_split=0.1, verbose=1, callbacks=[tensorboard_callback])\n",
        "# evaluate the model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "176/176 [==============================] - 248s 1s/step - loss: 0.6363 - accuracy: 0.6111 - val_loss: 0.4450 - val_accuracy: 0.8168\n",
            "Epoch 2/10\n",
            "176/176 [==============================] - 241s 1s/step - loss: 0.2860 - accuracy: 0.8822 - val_loss: 0.2818 - val_accuracy: 0.8792\n",
            "Epoch 3/10\n",
            "176/176 [==============================] - 242s 1s/step - loss: 0.1641 - accuracy: 0.9394 - val_loss: 0.4890 - val_accuracy: 0.8128\n",
            "Epoch 4/10\n",
            "176/176 [==============================] - 243s 1s/step - loss: 0.0822 - accuracy: 0.9733 - val_loss: 0.6543 - val_accuracy: 0.7864\n",
            "Epoch 5/10\n",
            "176/176 [==============================] - 242s 1s/step - loss: 0.0502 - accuracy: 0.9848 - val_loss: 0.5528 - val_accuracy: 0.8512\n",
            "Epoch 6/10\n",
            "176/176 [==============================] - 243s 1s/step - loss: 0.0337 - accuracy: 0.9896 - val_loss: 0.3610 - val_accuracy: 0.8924\n",
            "Epoch 7/10\n",
            "176/176 [==============================] - 243s 1s/step - loss: 0.0314 - accuracy: 0.9899 - val_loss: 0.5734 - val_accuracy: 0.8716\n",
            "Epoch 8/10\n",
            "176/176 [==============================] - 242s 1s/step - loss: 0.0168 - accuracy: 0.9952 - val_loss: 0.3724 - val_accuracy: 0.9084\n",
            "Epoch 9/10\n",
            "176/176 [==============================] - 243s 1s/step - loss: 0.0403 - accuracy: 0.9868 - val_loss: 0.5228 - val_accuracy: 0.8804\n",
            "Epoch 10/10\n",
            "176/176 [==============================] - 243s 1s/step - loss: 0.0072 - accuracy: 0.9978 - val_loss: 0.6698 - val_accuracy: 0.8572\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb313c516a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfKyS6SxqvEo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}